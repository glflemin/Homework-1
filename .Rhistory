dt1[-1,]
# Retreive a column
dt1[,1]
dt1[,-3]
dt1[,c(1:2)]
dt1[,"Last_Name"]
dt1[,c("Last_Name","Has_Phd?")]
dt1[,$Last_Name]
dt1$Last_Name
dt1[,"Last_Name"]
dt1$Last_Name
dt1$'Has_Phd?'
# Reassign an element
dt1[6,2] <- "R & Marketing Analytics"
dt1[6,]
# Filter by an value
dt1[dt1$`Has_Phd?`==1,]
# Filter by an value
dt1[dt1$`Has_Phd?`==1,]
# Add a new row - rbind (row-bind) function
dt1 <- rbind(dt1, c("Dasmohapatra", "Clustering", 1))
dt1
# Add a new column
dt1$First_Name <- c("Susan", "Shaina", "Christopher", "Aric", "Andrea", "Aaron", "Sudipta")
# Change the order of my columns
dt1 <- dt1[,c(4,1:3)]
dt1
# Make columns based off other columns
dt1$First_Name_Length <- nchar(dt1$First_Name)
dt1$Last_Name_Length <- nchar(dt1$Last_Name)
?nchar
dt1$Last_Name_Length <- nchar(dt1$Last_Name)
dt1$Total_Name_Length <- dt1$First_Name_Length + dt1$Last_Name_Length + 1
dt1$Full_Name <- paste(dt1$First_Name, " ", dt1$Last_Name)
dt1
?paste
# Remove some intermediate steps
dt1 <- dt1[,-c(5,6)]
# If you're unsure of what you have, use the class function:
total_add
class(total_add)
names
class(names)
l2
class(l2)
sequence_array_1_dim
class(sequence_array_1_dim)
A
class(A)
dt1
class(dt1)
dt1$First_Name
class(dt1$First_Name)
new_array <- as.array(total_add)
class(new_array)
old_vector <- as.vector(new_array)
class(old_vector)
?as.data.frame()
# Needed Libraries for Analysis #
install.packages('forecast',dependencies = T)
install.packages('tseries')
install.packages(c('expsmooth','lmtest','zoo','seasonal'))
install.packages('haven')
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
# Saving File Locations and Uploading SAS File #
file.dir <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\data\\Forecasting\\"
input.file1 <- "usairlines.sas7bdat"
input.file2 <- "ar2.sas7bdat"
USAirlines <- read_sas(paste(file.dir, input.file1,sep = ""))
AR2 <- read_sas(paste(file.dir, input.file2, sep = ""))
# Saving File Locations and Uploading SAS File #
file.dir <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\data\\Forecasting Data\\"
input.file1 <- "usairlines.sas7bdat"
input.file2 <- "ar2.sas7bdat"
USAirlines <- read_sas(paste(file.dir, input.file1,sep = ""))
AR2 <- read_sas(paste(file.dir, input.file2, sep = ""))
# Creation of Time Series Data Object #
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)
# Time Series Decomposition ...STL#
decomp_stl <- stl(Passenger, s.window = 7)
plot(decomp_stl)
plot(Passenger, col = "grey", main = "US Airline Passengers - Trend/Cycle", xlab = "", ylab = "Number of Passengers (Thousands)", lwd = 2)
lines(decomp_stl$time.series[,2], col = "red", lwd = 2)
decomp_stl$time.series
lines(decomp_stl$time.series[,2] )
rm(lines(decomp_stl$time.series[,2] ))
vdate< -seq(as.Date("2007-05-10 0:00"), by=1, len=3)
vdate< -seq(as.Date("2007-05-10 0:00"), by=1, len=3)
vdate <-seq(as.Date("2007-05-10 0:00"), by=1, len=3)
vdate
start <- as.Date("2007-05-10 0:00")
end <- as.Date("2018-06-13 0:00")
vdate <-seq(as.Date("2007-05-10 0:00"), by=(1/24), len=((end-start)/24))
vdate
start <- as.Datetime("2007-05-10 0:00")
end <- as.Datetime("2018-06-13 0:00")
vdate <-seq(start, by=(1/24), len=3)
vdate
start <- as.Datetime("2007-05-10 0:00")
end <- as.Datetime("2018-06-13 0:00")
vdate <-seq(start, by=(60*60), len=50)
vdate
vdate <-seq(start, by=1/24, len=50)
vdate
start <- as.Datetime("2007-05-10 0:00")
end <- as.Datetime("2018-06-13 0:00")
vdate <-seq(start, by=1/24, len=24*(start-end))
vdate <-seq(start, by=1/24, len=24*(end-start))
vdate
max(vdate)
tail(vdate)
start <- as.Datetime("2007-05-10 0:00", "%Y/%m/%d hh:mm")
end <- as.Datetime("2018-06-13 0:00", "%Y/%m/%d hh:mm")
start <- as.Date("2007-05-10 0:00", "%Y/%m/%d hh:mm")
end <- as.Date("2018-06-13 0:00", "%Y/%m/%d hh:mm")
vdate <-seq(start, by=1/24, len=24*(end-start))
start <- format(as.Datetime("2007-05-10 0:00"), "%Y/%m/%d hh:mm")
start <- format(as.Date("2007-05-10 0:00"), "%Y/%m/%d hh:mm")
end <- format(as.Date("2018-06-13 0:00"), "%Y/%m/%d hh:mm")
vdate <-seq(start, by=1/24, len=24*(end-start))
vdate <-seq(start, by=1/24, len=24*as.float(end-start))
start <- format(as.Date("2007-05-10 0:00"), "%Y/%m/%d %H:%M")
end <- format(as.Date("2018-06-13 0:00"), "%Y/%m/%d %H:%M")
vdate <-seq(start, by=1/24, len=24*as.double(end-start))
vdate <-seq(start, by=1/24, len=24*as.double(as.date(end)-as.date(start)))
vdate <-seq(start, by=1/24, len=24*(as.Date(end)-as.Date(start)))
tail(vdate)
tail(format(vdate, "%Y/%m/%d %H:%M"))
start <- as.Date("2007-05-10 0:00")
end <- as.Date("2018-06-13 0:00")
vdate <- format(seq(start, by=1/24, len=24*(end-start)), "%Y/%m/%d %H:%M")
tail(vdate)
time_range <- end-start
time_range
time_range <- format(end-start, "%Y/%m/%d %H")
start <- as.Date("2007-05-10 0:00")
end <- as.Date("2018-06-13 0:00")
time_range <- format(as.Date(end-start), "%Y/%m/%d %H:%M")
time_range <- format(end-start, "%Y/%m/%d %H:%M")
time_range <- difftime(x[1], x[2], units="hours")
time_range <- difftime(start, end, units="hours")
time_range
time_range <- difftime(end, start, units="hours")
time_range
time_range/(365*24)
vdate <- format(seq(start, by=1/24, len=97248, "%Y/%m/%d %H:%M")
vdate <- format(seq(start, by=1/24, len=97248), "%Y/%m/%d %H:%M")
vdate <- seq(start, by=1/24, len=97248)
vdate <- format(vdate, "%Y/%m/%d %H:%M")
tail(vdate)
vdate <- format(start, "%Y/%m/%d %H:%M")
start <- format(start, "%Y/%m/%d %H:%M")
vdate <- seq(start, by=1/24, len=97248)
vdate <- format(vdate, "%Y/%m/%d %H:%M")
start <- as.Datetime("2007-05-10 0:00")
end <- as.Datetime("2018-06-13 0:00")
vdate <- seq(
from=as.POSIXct("2007-05-10 0:00", tz="UTC"),
to=as.POSIXct("2018-06-13 0:00", tz="UTC"),
by="hour"
)
# time_range <- difftime(end, start, units="hours")
# time_range
# start <- format(start, "%Y/%m/%d %H:%M")
vdate <- seq(start, by=1/24, len=97248)
vdate <- seq(
from=as.POSIXct("2007-05-10 0:00", tz="UTC"),
to=as.POSIXct("2018-06-13 0:00", tz="UTC"),
by="hour"
)
# time_range <- difftime(end, start, units="hours")
# time_range
# start <- format(start, "%Y/%m/%d %H:%M")
# vdate <- seq(start, by=1/24, len=97248)
# vdate <- format(vdate, "%Y/%m/%d %H:%M")
tail(vdate)
# time_range <- difftime(end, start, units="hours")
# time_range
# start <- format(start, "%Y/%m/%d %H:%M")
# vdate <- seq(start, by=1/24, len=97248)
# vdate <- format(vdate, "%Y/%m/%d %H:%M")
tail(vdate)
time_range <- difftime(end, start, units="hours")
time_range
start <- as.Date("2007-05-10 0:00")
end <- as.Date("2018-06-13 0:00")
time_range <- difftime(end, start, units="hours")
time_range
start <- as.Date("2007-05-10 0:00")
end2 <- as.Date("2007-05-11 0:00")
time_range <- difftime(end2, start, units="hours")
time_range
start <- as.Date("2007-05-10 0:00")
end <- as.Date("2018-06-13 0:00")
time_range <- difftime(end2, start, units="hours")
time_range
time_range <- difftime(end, start, units="hours")
time_range
97/(365*24)
97000/(365*24)
start <- as.Date("2007-10-05 0:00")
end <- as.Date("2018-06-13 0:00")
time_range <- difftime(end, start, units="hours")
time_range
vdate <- seq(
from=as.POSIXct("2007-05-10 0:00", tz="UTC"),
to=as.POSIXct("2018-06-13 0:00", tz="UTC"),
by="hour"
)
start <- format(start, "%Y/%m/%d %H:%M")
# vdate <- seq(start, by=1/24, len=97248)
# vdate <- format(vdate, "%Y/%m/%d %H:%M")
tail(vdate)
library(readxl)
library(lubridate) #Library to manipulate dates
library(dplyr)
rm(list=ls())
# ensuring correct work directory
getwd()
# importing the Excel file
wbpath <- "G_561_T.xlsx"
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
#initialize lists for holding date-times and well depth data for first chuck of data (hourly section)
datetime1 = list(Sys.time())
well1 = list(0)
#Loop through hourly data and combine date and time into a single variable
for (i in 1:90755){
ye <- toString(year(G_561_T$date[i]))
mo <- toString(month(G_561_T$date[i]))
da <- toString(day(G_561_T$date[i]))
ho <- toString(hour(G_561_T$time[i]))
datet <- paste(ye, mo, da, ho, sep = " ")
datetime1[[i]] <- as.POSIXct(datet, format="%Y %m %d %H")
}
#Isolate well depth data for hourly data
well1 = G_561_T$Corrected[1:90755]
#Initialize counter variables and lists for the 15-min increment data
j=1
k=1
depths = c(0, 0, 0, 0)    #vector variable that holds the depths to be averaged for the hour
well2 <- list(0)
hold <- c(year(G_561_T$date[90756]), month(G_561_T$date[90756]), day(G_561_T$date[90756]), hour(G_561_T$time[90756]))  #initialize first date/hour
datetime2 = list(Sys.time())
#Loop through 15-min data
for (i in 90756:101547){
current <- c(year(G_561_T$date[i]), month(G_561_T$date[i]), day(G_561_T$date[i]), hour(G_561_T$time[i]))  #get current date/hour
if (hold[1] == current[1] & hold[2] == current[2] & hold[3] == current[3] & hold[4] == current[4]){   #check if hold and current date/hour are the same
depths[j] <- G_561_T$Corrected[i]   #if the date/hour are the same, add the well depth to list of depths for the hour
j=j+1
}
if (i == 101547){                     #if the loop is at the last entry, calculate average depth and add corresponding datetime to list
well2[k] <- sum(depths)/(j-1)
ye <- toString(year(G_561_T$date[i]))
mo <- toString(month(G_561_T$date[i]))
da <- toString(day(G_561_T$date[i]))
ho <- toString(hour(G_561_T$time[i]))
datet <- paste(ye, mo, da, ho, sep = " ")
datetime2[[k]] <- as.POSIXct(datet, format="%Y %m %d %H")
} else if (hour(G_561_T$time[i]) != hour(G_561_T$time[i+1])){    #if the next date/hour is different, average depth, record date/hour, and reset appropriate variables
hold <- c(year(G_561_T$date[i+1]), month(G_561_T$date[i+1]), day(G_561_T$date[i+1]), hour(G_561_T$time[i+1]))
well2[k] <- sum(depths)/(j-1)
j=1
depths <- c(0,0,0,0)
ye <- toString(year(G_561_T$date[i]))
mo <- toString(month(G_561_T$date[i]))
da <- toString(day(G_561_T$date[i]))
ho <- toString(hour(G_561_T$time[i]))
datet <- paste(ye, mo, da, ho, sep = " ")
datetime2[[k]] <- as.POSIXct(datet, format="%Y %m %d %H")
k=k+1
}
}
#combine the two halves of the dates and well depths
well <- unlist(c(well1, well2))
datetime <- c(datetime1, datetime2)
#combine the two variables into a dataframe
new_well_data <- data.frame(as.POSIXct(unlist(datetime), origin = "1970-01-01"), well)
colnames(new_well_data)[1] <- 'date_time'
#calculate mean and stdev of well variable
meandepth = mean(well)
stdevdepth = sd(well)
# ensuring correct work directory
getwd()
# ensuring correct work directory
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
rm(list=ls())
# ensuring correct work directory
this.dir <- dirname(parent.frame(2)$ofile)
this.dir
# ensuring correct work directory
this.dir <- dirname(parent.frame(2)$ofile)
this.dir <- setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(rstudioapi)
install.packages('rstudioapi')
library(rstudioapi)
this.dir <- setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(this.dir)
getwd()
this.dir <- setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(this.dir)
getwd()
# importing the Excel file
wbpath <- paste(getwd(),"\\", "G_561_T.xlsx",sep="")
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
# importing the Excel file
wbpath <- paste(getwd(),"/", "G_561_T.xlsx",sep="")
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
# importing the Excel file
wbpath <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\G_561_T.xlsx"
getwd() # checking current work directory
# importing the Excel file
wbpath <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\G_561_T.xlsx"
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
wbpath <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\G_561_T.xlsx"
library(readxl)
library(lubridate) #Library to manipulate dates
library(dplyr)
rm(list=ls())
getwd() # checking current work directory
# importing the Excel file
wbpath <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\G_561_T.xlsx"
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
#initialize lists for holding date-times and well depth data for first chuck of data (hourly section)
datetime1 = list(Sys.time())
well1 = list(0)
#Loop through hourly data and combine date and time into a single variable
for (i in 1:90755){
ye <- toString(year(G_561_T$date[i]))
mo <- toString(month(G_561_T$date[i]))
da <- toString(day(G_561_T$date[i]))
ho <- toString(hour(G_561_T$time[i]))
datet <- paste(ye, mo, da, ho, sep = " ")
datetime1[[i]] <- as.POSIXct(datet, format="%Y %m %d %H")
}
#Isolate well depth data for hourly data
well1 = G_561_T$Corrected[1:90755]
#Initialize counter variables and lists for the 15-min increment data
j=1
k=1
depths = c(0, 0, 0, 0)    #vector variable that holds the depths to be averaged for the hour
well2 <- list(0)
hold <- c(year(G_561_T$date[90756]), month(G_561_T$date[90756]), day(G_561_T$date[90756]), hour(G_561_T$time[90756]))  #initialize first date/hour
datetime2 = list(Sys.time())
#Loop through 15-min data
for (i in 90756:101547){
current <- c(year(G_561_T$date[i]), month(G_561_T$date[i]), day(G_561_T$date[i]), hour(G_561_T$time[i]))  #get current date/hour
if (hold[1] == current[1] & hold[2] == current[2] & hold[3] == current[3] & hold[4] == current[4]){   #check if hold and current date/hour are the same
depths[j] <- G_561_T$Corrected[i]   #if the date/hour are the same, add the well depth to list of depths for the hour
j=j+1
}
if (i == 101547){                     #if the loop is at the last entry, calculate average depth and add corresponding datetime to list
well2[k] <- sum(depths)/(j-1)
ye <- toString(year(G_561_T$date[i]))
mo <- toString(month(G_561_T$date[i]))
da <- toString(day(G_561_T$date[i]))
ho <- toString(hour(G_561_T$time[i]))
datet <- paste(ye, mo, da, ho, sep = " ")
datetime2[[k]] <- as.POSIXct(datet, format="%Y %m %d %H")
} else if (hour(G_561_T$time[i]) != hour(G_561_T$time[i+1])){    #if the next date/hour is different, average depth, record date/hour, and reset appropriate variables
hold <- c(year(G_561_T$date[i+1]), month(G_561_T$date[i+1]), day(G_561_T$date[i+1]), hour(G_561_T$time[i+1]))
well2[k] <- sum(depths)/(j-1)
j=1
depths <- c(0,0,0,0)
ye <- toString(year(G_561_T$date[i]))
mo <- toString(month(G_561_T$date[i]))
da <- toString(day(G_561_T$date[i]))
ho <- toString(hour(G_561_T$time[i]))
datet <- paste(ye, mo, da, ho, sep = " ")
datetime2[[k]] <- as.POSIXct(datet, format="%Y %m %d %H")
k=k+1
}
}
#combine the two halves of the dates and well depths
well <- unlist(c(well1, well2))
datetime <- c(datetime1, datetime2)
#combine the two variables into a dataframe
new_well_data <- data.frame(as.POSIXct(unlist(datetime), origin = "1970-01-01"), well)
colnames(new_well_data)[1] <- 'date_time'
#calculate mean and stdev of well variable
meandepth = mean(well)
stdevdepth = sd(well)
#Library to manipulate dates and excel and dataframes
#if you don't have any of these, install them
library(readxl)
library(lubridate)
library(dplyr)
# for cleaning global environment
#rm(list=ls())
# ensuring correct work directory
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# importing the Excel file
wbpath <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\G_561_T.xlsx"
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
#building ideal vector of dates
#necessary to convert it to data.frame for merge later
vdate <- data.frame(seq(
from=as.POSIXct("2007-10-05 0:00", tz="EST"),
to=as.POSIXct("2018-06-13 0:00", tz="EST"),
by="hour"
) )
length(vdate[[1]])
colnames(vdate) <- 'date_time' #renaming vector to match other merge datafram
# Rounding dates to their hour, Frankensteining date & hour together, then converting to POSIX
# Assumes all times are EST
# TODO: Correct EST assumption for timestamps in EDT
G_561_T$date_time <- paste(G_561_T$date," ", lubridate::hour(G_561_T$time),":00:00", sep = "")
G_561_T$date_time <- as.POSIXct(G_561_T$date_time, tz="EST")
#grouping water levels by taking average of each hour
clean_well <- G_561_T %>%
group_by(date_time) %>%
summarise(mean_corr=mean(Corrected)) %>%
select(date_time, mean_corr)
#THE MERGES
#If you didn't run 'Bill's Code.R' prior to this, you'll hit errors here.
final_P <- left_join(vdate, clean_well, by='date_time')
final_J <- left_join(vdate, new_well_data, by='date_time')
# Identifying missing values
missing_P <- filter(final_P, is.na(mean_corr))
missing_J <- filter(final_J, is.na(well))
missing_P_only <- anti_join(missing_P, missing_J, by='date_time')
missing_J_only <- anti_join(missing_J, missing_P, by='date_time')#identifying discrepancies
nrow(missing_P_only)
nrow(missing_J_only) # UGH why don't nrow(missing_P_only) and nrow(missing_J_only) add up to 44!?!?
View(missing_P_only)
View(missing_J_only)
length(new_well_data$well) #Jenista 93486
length(vdate[[1]]) #Ideal 93697
length(clean_well$mean_corr) #Powell 93442
length(missing_J[[1]]) #211
length(missing_P[[1]]) #255
length(final_P$mean_corr)-length(vdate[[1]]) #confirming merge created expected length
# Taking summaries of both datasets for comparison, possible trends
# Powell Dataset...taking averages of averages in the mean function, but whatever
final_P_summary <- final_P %>%
group_by(year=lubridate::year(date_time), month=lubridate::month(date_time)) %>%
summarize(mean = mean(mean_corr, na.rm = TRUE), count = n())
# Jenista Dataset
final_J_summary <- final_J %>%
group_by(year=lubridate::year(date_time), month=lubridate::month(date_time)) %>%
summarize(mean = mean(well, na.rm = TRUE), count = n())
#subtracting: Jenista - Powell to identify areas of error
comparison <- final_P_summary - final_J_summary
View(comparison)
sum(comparison$count) # distributed diffences match the previous difference(sum=44)
comparison$month <- final_P_summary$month
comparison$year <- final_P_summary$year
View(comparison)
View(final_J)
View(final_P)
#Library to manipulate dates and excel and dataframes
#if you don't have any of these, install them
library(readxl)
library(lubridate)
library(dplyr)
library(zoo)
# for cleaning global environment
rm(list=ls())
# ensuring correct work directory
#this.dir <- dirname(parent.frame(2)$ofile)
#setwd(this.dir)
#setwd('C:\\Users\\gavin\\Desktop\\Time_Series_Data\\')
setwd("C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\")
# importing the Excel file
wbpath <- "C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\lab and hw\\Time Series\\HW1\\Homework-1\\G_561_T.xlsx"
#wbpath <- "C:\\Users\\gavin\\Desktop\\Time_Series_Data\\G-561_T.xlsx"
G_561_T <- read_excel(wbpath, sheet=3) # need the full filepath to make this work
#building ideal vector of dates
#necessary to convert it to data.frame for merge later
vdate <- data.frame(seq(
from=as.POSIXct("2007-10-05 0:00", tz="EST"),
to=as.POSIXct("2018-06-13 0:00", tz="EST"),
by="hour"
) )
length(vdate[[1]])
colnames(vdate) <- 'date_time' #renaming vector to match other merge datafram
# Rounding dates to their hour, Frankensteining date & hour together, then converting to POSIX
# Assumes all times are EST
# TODO: Correct EST assumption for timestamps in EDT
G_561_T$date_time <- paste(G_561_T$date," ", lubridate::hour(G_561_T$time),":00:00", sep = "")
G_561_T$date_time <- as.POSIXct(G_561_T$date_time, tz="EST")
#grouping water levels by taking average of each hour
clean_well <- G_561_T %>%
group_by(date_time) %>%
summarise(mean_corr=mean(Corrected)) %>%
select(date_time, mean_corr)
#calc avg stdev
meandepth = mean(clean_well$mean_corr, na.rm=TRUE)
stdevdepth = sd(clean_well$mean_corr, na.rm=TRUE)
#THE MERGE
final_df <- left_join(vdate, clean_well, by='date_time')
rm(list=setdiff(ls(), "final_df"))
str(final_df)
#Creation of Time Series Data Object
df <- ts(final_df$mean_corr, start = c(2007,10,5,0), frequency = 8760)
# Time Series Decomposition ...STL# #STL=Seasonal, Trend, Low S
decomp_stl <- stl(df, s.window = 7, na.action = na.approx)
#Depth= time series object,
#s.window you have to have this, and it should be odd and no less than 7.  Moving average.
#Plot Decomposition
plot(decomp_stl)
plot.ts(df, xlab = "Year", ylab = "Depth (Ft)")
plot(df, xlab = "Year", ylab = "Depth (Ft)")
plot(df, col = "grey", main = "Well Depth - Trend/Cycle", xlab = "Year", ylab = "Depth (Feet) ", lwd = 2)
lines(decomp_stl$time.series[,2], col = "red", lwd = 2)#plotting the trend line on the time series data
